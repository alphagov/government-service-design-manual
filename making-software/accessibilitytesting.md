---
layout: detailed-guidance
title: Accessibility testing
subtitle: Testing to see if your service is inclusive
section: guidance
subsection: Accessibility 
type: guide
audience: 
    primary: researcher
    secondary: designer, developer
theme: Measurement
status: draft
assets: local
---

This guidance looks at how accessibility testing should be conducted to ensure that new services are built with all users in mind.

##Guidance

Accessibility testing is very similar to usability testing, in that it is about ensuring that a product or service is easy to use for it’s intended audience. That audience includes ‘disabled’ users who may access the service via a range of assistive technologies such as screen readers, voice recognition software, trackball devices and so on.

It’s important to consider a range of disabilities when you are testing any product or service, including those with;

* cognitive and learning disabilities e.g. dyslexia or attention deficit disorders
* visual impairments e.g. total and partial blindness, colour blindness, poor vision 
* auditory disabilities which can also affect language 
* motor skills impairments e.g. those affected by arthritis, strokes, RSI

Section III of the Disability Discrimination Act (DDA), also states that websites should be accessible to blind and disabled users. The Code of Practice for this section of the DDA was published on 27th May 2002. The elements most relevant to website designs are set out in [this blogpost](http://www.webcredible.co.uk/user-friendly-resources/web-accessibility/uk-website-legal-requirements.shtml).

Accessibility audits are an alternative to standard accessibility testing. An accessibility audit involves an accessibility expert reviewing the site or service, highlighting all accessibility issues and making recommendations for fixing them. They would typically use assistive software used by disabled web users (e.g. a screen reader) to effectively carry out the audit. See the [W3C accessibility guidelines](http://www.w3.org/TR/WCAG/) for further information.

Accessibility audits are cheaper and quicker than accessibility testing but rely primarily on the expertise of the person conducting them.


##Where and how you might use it

Most accessibility testing is typically conducted after an accessibility audit has been conducted.

Accessibility testing with ‘real’ participants with a range of ‘disabilities’ is best conducted in the participants' own homes. This is because they will often have things set up to suit their individual needs and the whole process is less stressful for them e.g. travel, environment.

##When not to use it

It is often difficult to conduct accessibility testing early on in the process of service design. The service must be fairly robust in order for it to be evaluated by people using assistive technologies e.g. a screen reader, such as Jaws, will read out the contents of a web page so the code needs to be well structured. For accessibility testing to be worth doing, real content needs to be in place rather than ‘dummy text’ if it is to be assessed by those with any cognitive or learning difficulties. Interactive elements such as Calls To Action, hyperlinks, forms etc must be in place if motor skills are being assessed.

Full lab-based accessibility testing is not necessary for every project. An accessibility audit may be a more efficient and cost-effective way to review a service, depending on it's typical user needs.

##Types of participants

Disabled participants should be included as part of a wider user testing recruitment process. The numbers will be small, but should aim to capture a range of disabilities and ‘assistive technologies’.  

##Cost

This is a harder to reach audience so the cost of doing so can be relatively expensive. Recruitment is best conducted through specialist organisations or agencies e.g. AbilityNet, RNIB, etc

Additional costs can be incurred if these participants are travelling to your testing location and/or require specialist assistance with carers or travel.

##Timescales
Recruitment via an agency can take up to 2 weeks, depending on the target audience. 
Conducting testing sessions can take between 2-3 days depending on the number of participants. This may vary depending on whether the sessions are lab-based or structured sessions in a ‘home environment’. Analysis and reporting should take up to a week.
These estimates are dependent on the project's scope, and the availability of ‘robust’ testing assets.
